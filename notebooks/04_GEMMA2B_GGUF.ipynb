{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Gemma 2B ‚Äì Quantizing GGUF\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** February 01, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook documents the correct pipeline for converting a **LoRA-fine-tuned (QLoRA)** model into a **quantized GGUF** model, ready for use in local runtimes such as **llama.cpp**, **Jan**, **Ollama**, and **LM Studio**.\n",
    "\n",
    "‚ö†Ô∏è An important note: **It is not possible to quantize a LoRA model directly**.  \n",
    "\n",
    "The process requires a well-defined sequence of intermediate steps.\n",
    "\n",
    "### Correct pipeline order\n",
    "\n",
    "- **Merge the LoRA adapter with the base model**  \n",
    "‚Üí Produces a full model in **fp16 or bfloat16**\n",
    "\n",
    "- **Convert Hugging Face to GGUF (fp16)**  \n",
    "‚Üí Using the official `llama.cpp` tooling\n",
    "\n",
    "- **Quantize the GGUF model**  \n",
    "‚Üí Formats such as **Q4 / Q5 / Q8**, depending on the desired balance between quality and memory usage\n",
    "\n",
    "Every **GGUF model is first created in fp16**. Quantization is only applied after this conversion step.\n",
    "\n",
    "Throughout this notebook, this workflow is applied to a **Gemma 2B** model fine-tuned for empathetic and psychologically safe responses, covering everything from LoRA merging to final quantization, inference testing, and publication to Hugging Face.\n",
    "\n",
    "The final merged model is publicly available on the Hugging Face Hub at:\n",
    "\n",
    "- **https://huggingface.co/ecorbari/Gemma-2b-it-Psych-GGUF**\n",
    "\n",
    "### Alternative: GGUF Conversion Without Local Setup\n",
    "\n",
    "If you prefer **not to perform the conversion and quantization locally**, Hugging Face provides a simple and fully managed alternative via the following Space:\n",
    "\n",
    "- **https://huggingface.co/spaces/ggml-org/gguf-my-repo**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Prerequisites\n",
    "\n",
    "If the previous notebook steps were executed successfully, the following model directory should already exist:\n",
    "\n",
    "### Expected result\n",
    "\n",
    "Gemma-2b-it-Psych-Merged/  \n",
    "‚îú‚îÄ config.json  \n",
    "‚îú‚îÄ model.safetensors  \n",
    "‚îú‚îÄ tokenizer.model / tokenizer.json  \n",
    "\n",
    "If this directory does not exist, you must download the merged model from:\n",
    "\n",
    "- https://huggingface.co/ecorbari/Gemma-2b-it-Psych-Merged\n",
    "\n",
    "The `tokenizer.model` file is also required and can be obtained from:\n",
    "\n",
    "- https://huggingface.co/google/gemma-2b-it/blob/main/tokenizer.model\n",
    "\n",
    "After downloading, move the tokenizer file into the model directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "mv tokenizer.model ~/fine-tune-llm/notebooks/Gemma-2b-it-Psych-Merged/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install / Update llama.cpp\n",
    "\n",
    "You can use the official `llama.cpp` repository as a reference to install the runtime on your machine.  \n",
    "It is important that your system has a **CUDA-capable GPU** available.\n",
    "\n",
    "A detailed guide on installing and configuring llama.cpp is available here:\n",
    "\n",
    "- [Build llama-cpp](https://ecorbari.medium.com/running-local-llms-on-ubuntu-with-nvidia-gpu-using-llama-cpp-2ec2e010c040)\n",
    "\n",
    "Below are the essential steps to build `llama.cpp` with CUDA support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p ~/projects && cd ~/projects\n",
    "git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure and build with **CUDA** enabled (adjust the CUDA architecture according to your GPU compute capability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "cmake -B build \\\n",
    "  -DGGML_CUDA=ON \\\n",
    "  -DLLAMA_CURL=ON \\\n",
    "  -DCMAKE_CUDA_ARCHITECTURES=75  # Use your GPU compute capability (without decimals)\n",
    "\n",
    "cmake --build build -j$(nproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid dependency conflicts, create a Python virtual environment inside the `llama.cpp` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "python3 -m venv venv && source venv/bin/activate\n",
    "pip install --upgrade pip && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Converting the Model to GGUF Format\n",
    "\n",
    "With the merged model ready, the next step is to convert it to the **GGUF format**, which is required before any quantization can be applied.\n",
    "\n",
    "Run the following command from the `llama.cpp` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "python3 convert_hf_to_gguf.py \\\n",
    "  ~/fine-tune-llm/notebooks/Gemma-2b-it-Psych-Merged/ \\\n",
    "  --outfile gemma-2b-it-psych-f16.gguf \\\n",
    "  --outtype f16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command converts the merged Hugging Face model into a GGUF file in **fp16** precision.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "After a successful conversion, the following file will be generated:\n",
    "\n",
    "`gemma-2b-it-psych-f16.gguf`\n",
    "\n",
    "You can verify the file size with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "du -sh gemma-2b-it-psych-f16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "- 4.7G gemma-2b-it-psych-f16.gguf\n",
    "\n",
    "üìå This fp16 GGUF file serves as the base artifact for all subsequent quantization steps (e.g., Q4, Q5, Q8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Quantizing to GGUF\n",
    "\n",
    "With the fp16 GGUF file generated, we can now apply quantization to significantly reduce the model size while preserving most of its performance.\n",
    "\n",
    "**Recommended quantization:**\n",
    "\n",
    "The following command applies **Q5_K_M quantization**, which offers an excellent balance between model quality and memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "./build/bin/llama-quantize \\\n",
    "  gemma-2b-it-psych-f16.gguf \\\n",
    "  gemma-2b-it-psych-q5_k_m.gguf \\\n",
    "  Q5_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After quantization, the resulting file will be noticeably smaller:\n",
    "\n",
    "- 1.8G gemma-2b-it-psych-q5_k_m.gguf\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "Other quantization formats (e.g., Q4, Q6, Q8) can also be used depending on your deployment constraints.\n",
    "\n",
    "**Q5_K_M** is generally recommended for instruction-following and alignment-sensitive models, as it provides a strong balance between compactness and output quality.\n",
    "\n",
    "üìå The quantized GGUF file is now ready for efficient local inference using llama.cpp, Jan, Ollama, or LM Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Testing Model Inference\n",
    "\n",
    "With the quantized GGUF model ready, the next step is to validate inference and ensure the model behaves as expected.\n",
    "\n",
    "### Single-prompt inference\n",
    "\n",
    "The following command runs a single inference pass using a fixed prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "./build/bin/llama-cli \\\n",
    "  -m gemma-2b-it-psych-q5_k_m.gguf \\\n",
    "  -p \"I feel anxious and overwhelmed lately. What should I do?\" \\\n",
    "  -n 256 \\\n",
    "  --temp 0.7 \\\n",
    "  --repeat-penalty 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "This allows you to quickly verify:\n",
    "\n",
    "- Model loading and runtime compatibility\n",
    "- Output coherence and alignment\n",
    "- Basic response quality after quantization\n",
    "\n",
    "**Interactive chat mode**\n",
    "\n",
    "For a real-time, conversational experience with the model, use interactive chat mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "./build/bin/llama-cli \\\n",
    "  -m gemma-2b-it-psych-q5_k_m.gguf \\\n",
    "  -cnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mode enables continuous dialogue, making it easier to evaluate:\n",
    "\n",
    "- Instruction-following behavior\n",
    "- Conversational consistency\n",
    "- Empathy and tone across multiple turns\n",
    "\n",
    "üìå At this point, the model is fully operational and ready for practical use in local inference environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Publishing the Model to Hugging Face\n",
    "\n",
    "After validating inference, the final step is to publish the quantized GGUF model to the **Hugging Face Hub**, making it easily accessible for download and integration with local runtimes.\n",
    "\n",
    "Before uploading, authenticate with the Hugging Face CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the prompts and provide your Hugging Face access token.\n",
    "\n",
    "**Upload the GGUF model**\n",
    "\n",
    "Use the following command to upload the quantized model file to an existing Hugging Face repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "hf upload \\\n",
    "  ecorbari/Gemma-2b-it-Psych-GGUF \\\n",
    "  /home/edmc/projects/llama.cpp/gemma-2b-it-psych-q5_k_m.gguf \\\n",
    "  --repo-type model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once uploaded, the model becomes publicly available and can be consumed directly by tools that support GGUF models, such as llama.cpp, Ollama, LM Studio, and Jan (local mode).\n",
    "\n",
    "üìå For additional details on the Hugging Face CLI, refer to the official documentation:\n",
    "\n",
    "- https://huggingface.co/docs/huggingface_hub/en/guides/cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the Model with Jan**\n",
    "\n",
    "To use the published GGUF model in Jan, configure it as a local model (GGUF / llama.cpp backend).\n",
    "Jan does not list GGUF models under the **Hugging Face** provider, but it fully supports GGUF files in local mode.\n",
    "\n",
    "For more information and downloads, visit:\n",
    "\n",
    "- https://www.jan.ai/\n",
    "\n",
    "At this point, the model is fully packaged, published, and ready for practical use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
