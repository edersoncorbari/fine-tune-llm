{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFqkDeQ6nOC6"
   },
   "source": [
    "# üìò Gemma 2B ‚Äì Fine Tunning\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** January 25, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **lightweight smoke** test for loading, fine-tuning, and running inference with the **Gemma 2B Large Language Model (LLM)** using a psychological preference dataset.\n",
    "\n",
    "The dataset is structured for preference-based or comparative training, enabling the model to learn behavioral alignment by favoring psychologically safe and therapeutic responses over misaligned ones, rather than relying on standard supervised fine-tuning.\n",
    "\n",
    "The final merged model is publicly available on the Hugging Face Hub at:\n",
    "\n",
    "- **https://huggingface.co/ecorbari/Gemma-2b-it-Psych**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cLDxnooXEC"
   },
   "source": [
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "This notebook performs preference-based fine-tuning of the Gemma 2B model and includes a series of checks to ensure the training and inference pipeline is correctly configured.\n",
    "\n",
    "The primary goals are to validate:\n",
    "\n",
    "- Model and tokenizer loading\n",
    "- Proper handling of preference/comparison data\n",
    "- Inference behavior aligned with therapeutic and empathetic objectives\n",
    "- Environment and dependency correctness\n",
    "\n",
    "Although intentionally minimal, this notebook serves as a starting point for:\n",
    "\n",
    "- Preference-based fine-tuning approaches (e.g., DPO-style methods)\n",
    "- Behavioral alignment and safety evaluation\n",
    "- Prompt engineering in psychological and therapeutic contexts\n",
    "- Rapid experimentation prior to larger-scale alignment pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHait__RosVb"
   },
   "source": [
    "## 2Ô∏è‚É£ Environment & Dependencies\n",
    "\n",
    "This notebook assumes:\n",
    "- PyTorch with CUDA support\n",
    "- Hugging Face Transformers\n",
    "- bitsandbytes (for 4-bit quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers --quiet\n",
    "%pip install -U datasets --quiet\n",
    "%pip install -U accelerate --quiet\n",
    "%pip install -U peft --quiet\n",
    "%pip install -U trl --quiet\n",
    "%pip install -U bitsandbytes --quiet\n",
    "%pip install -U flash-attn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from typing import Final, List, Set, Type, Dict, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA T1000 8GB\n",
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"GPU CUDA not found\"\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME: Final[str] = \"google/gemma-2b-it\"\n",
    "DATASET_NAME: Final[str] = \"jkhedri/psychology-dataset\"\n",
    "\n",
    "# Set to None to use the full dataset\n",
    "DATA_SAMPLES: int | None = None\n",
    "\n",
    "NEW_MODEL: Final[str] = \"Gemma-2b-it-Psych\"\n",
    "SYSTEM_PROMPT: Final[str] = \"You are a compassionate mental health assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmz4qFaSuxIq"
   },
   "source": [
    "## 3Ô∏è‚É£ GPU Optimization\n",
    "\n",
    "Checks CUDA compute capability to automatically select the most efficient data type (`bfloat16` vs `float16`) and attention mechanism (`Flash Attention 2` vs `Eager`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUDA] 7.5 ‚Üí torch.float16, eager\n"
     ]
    }
   ],
   "source": [
    "major, minor = torch.cuda.get_device_capability()\n",
    "\n",
    "torch_dtype, attn_implementation = (\n",
    "    (torch.bfloat16, \"flash_attention_2\")\n",
    "    if major >= 8\n",
    "    else (torch.float16, \"eager\")\n",
    ")\n",
    "\n",
    "print(f\"[CUDA] {major}.{minor} ‚Üí {torch_dtype}, {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLRFbfi1vDDa"
   },
   "source": [
    "## 4Ô∏è‚É£ Quantization Configuration\n",
    "\n",
    "Sets up 4-bit quantization using bitsandbytes (QLoRA). It uses **NF4** (NormalFloat4) and **Double Quantization** to maximize VRAM efficiency while maintaining model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTJTc7w0vOts"
   },
   "source": [
    "## 5Ô∏è‚É£ Load Model and Tokenizer\n",
    "\n",
    "Loads the pre-trained model with quantization (4-bit/8-bit) and configures the tokenizer for SFT (Supervised Fine-Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3c4d5e0c434985a989d41eb351bad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E56yYs4svYnB"
   },
   "source": [
    "## 6Ô∏è‚É£ Target Module Discovery\n",
    "\n",
    "Automatically identifies all 4-bit linear layers within the model architecture to apply LoRA adapters, excluding the output layer (`lm_head`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(\n",
    "    model: nn.Module,\n",
    "    linear_cls: Type[nn.Module] = bnb.nn.Linear4bit,\n",
    "    exclude: Set[str] | None = {\"lm_head\"},\n",
    ") -> List[str]:\n",
    "    names = {\n",
    "        name.split(\".\")[-1]\n",
    "        for name, module in model.named_modules()\n",
    "        if isinstance(module, linear_cls)\n",
    "    }\n",
    "\n",
    "    return sorted(names - (exclude or set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(f\"LoRA modules: {modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1OEBdcousTj"
   },
   "source": [
    "## 7Ô∏è‚É£ LoRA Adapter Configuration\n",
    "\n",
    "Configures the LoRA parameters (Rank and Alpha) and attaches the adapters to the target modules. It also displays the number of trainable parameters, showing the efficiency of PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDUx-6YfwGLm"
   },
   "source": [
    "## 8Ô∏è‚É£ Dataset Loading\n",
    "\n",
    "Loads and shuffles the dataset with a toggle for **Test Mode**. If `DATA_SAMPLES` is set, it limits the data to a small subset for fast iteration before running the full fine-tuning.\n",
    "\n",
    "Applies a standardized chat template to the training and test sets. This process:\n",
    "\n",
    "1. **Wraps** user questions and system prompts into the model's specific conversation format.\n",
    "2. **Pairs** them with the psychological responses (`response_j`).\n",
    "3. **Cleans** the dataset by removing raw columns and keeping only the formatted `text` for training.\n",
    "\n",
    "**Note:** This is a preference/comparison dataset where `response_j` (empathetic/therapeutic) and `response_k` (judgmental/aggressive) represent opposite poles.\n",
    "\n",
    "We are specifically selecting **`response_j`** for training to ensure the model learns safe, professional, and supportive psychological guidance while explicitly avoiding the toxic patterns found in `response_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65)\n",
    "\n",
    "if DATA_SAMPLES is not None:\n",
    "    dataset = dataset.select(range(DATA_SAMPLES))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(\n",
    "    row: Dict[str, Any],\n",
    "    *,\n",
    "    tokenizer,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    ") -> Dict[str, Any]:\n",
    "    user_content = f\"{system_prompt}\\n\\n{row['question']}\"\n",
    "\n",
    "    messages = (\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response_j\"]},\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **row,\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8038fe5a8f4a82a6aa6f405abff3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d218d65415c7435eb9c75a3ef46d78bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_split(split: Dataset) -> Dataset:\n",
    "    return split.map(\n",
    "        lambda row: format_chat_template(row, tokenizer=tokenizer),\n",
    "        remove_columns=split.column_names,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "dataset[\"train\"] = map_split(dataset[\"train\"])\n",
    "dataset[\"test\"] = map_split(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'response_j', 'response_k', 'text'],\n",
       "        num_rows: 8861\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'response_j', 'response_k', 'text'],\n",
       "        num_rows: 985\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmYll9nf-ahe"
   },
   "source": [
    "## 9Ô∏è‚É£ Training\n",
    "\n",
    "Defines the execution strategy, including **Gradient Accumulation** to simulate larger batches on limited VRAM, **Paged AdamW** optimizer for memory stability, and logging/evaluation intervals to monitor the model's psychological alignment.\n",
    "\n",
    "After initializes the **SFTTrainer** by combining the model, processed dataset, LoRA configuration, and training arguments. This step starts the supervised fine-tuning process, optimizing the model to generate empathetic psychological responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=NEW_MODEL,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f55acb6e70941f7ad46d2563b400806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76483b4e427d4979a8558c6a21759635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667cf48b5e84f6886f7360e5671824b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51ae24231754f0fac08f23c97d2492a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1196e96f6ca4f81911e6ea5703d2fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b8eaf9948849ccb754c9562be3a8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4431' max='4431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4431/4431 7:40:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.846223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.843432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.796147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.851856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.758197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.763127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.758546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.728880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.715866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.598600</td>\n",
       "      <td>0.704105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.668521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.672166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.654271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.642203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.637317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.587400</td>\n",
       "      <td>0.626932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.622499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.561500</td>\n",
       "      <td>0.604518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.603095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>0.598177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.597766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4431, training_loss=0.7069022529242512, metrics={'train_runtime': 27632.8753, 'train_samples_per_second': 0.321, 'train_steps_per_second': 0.16, 'total_flos': 9628665165950976.0, 'train_loss': 0.7069022529242512})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkX5HqH0-v9-"
   },
   "source": [
    "## 1Ô∏è‚É£0Ô∏è‚É£ Training Visualization\n",
    "\n",
    "Initializes **TensorBoard** to monitor training metrics in real time. This allows tracking the loss curve, additional metrics, and verifying that the model is converging correctly during the fine-tuning process.\n",
    "\n",
    "- **Option 1**: Using TensorBoard inside a Notebook (Jupyter / Colab)\n",
    "\n",
    "Run the following cells:\n",
    "\n",
    "```bash\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./Gemma-2b-it-Psych\n",
    "```\n",
    "\n",
    "- **Option 2**: Using the Terminal + Web Browser\n",
    "\n",
    "Open a terminal at the project root and run:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ./notebooks/Gemma-2b-it-Psych\n",
    "```\n",
    "\n",
    "Then open your browser and navigate to:\n",
    "\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eULmfvrv_4oV"
   },
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Saving and Exporting the Model\n",
    "\n",
    "Persists the fine-tuned LoRA adapters to local storage and uploads the final weights to the **Hugging Face Hub**. This ensures the model is versioned and ready for deployment or future inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf71bba07b404078a34a5564b9cf8d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c9dcd040d74744b2b9ee1a626c5e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca219231264432b83ab4c332ca7fad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ecorbari/Gemma-2b-it-Psych/commit/f3f02ec9d968f4adc99c191955f7176f0f3cba5c', commit_message='Upload model', commit_description='', oid='f3f02ec9d968f4adc99c191955f7176f0f3cba5c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ecorbari/Gemma-2b-it-Psych', endpoint='https://huggingface.co', repo_type='model', repo_id='ecorbari/Gemma-2b-it-Psych'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(NEW_MODEL)\n",
    "trainer.model.push_to_hub(NEW_MODEL, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
