{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFqkDeQ6nOC6"
   },
   "source": [
    "# üìò Gemma 2B ‚Äì Fine Tunning\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** January 24, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **lightweight smoke** test for loading, fine-tuning, and running inference with the **Gemma 2B Large Language Model (LLM)** using a psychological preference dataset.\n",
    "\n",
    "The dataset is structured for preference-based or comparative training, enabling the model to learn behavioral alignment by favoring psychologically safe and therapeutic responses over misaligned ones, rather than relying on standard supervised fine-tuning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cLDxnooXEC"
   },
   "source": [
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "This notebook performs preference-based fine-tuning of the Gemma 2B model and includes a series of checks to ensure the training and inference pipeline is correctly configured.\n",
    "\n",
    "The primary goals are to validate:\n",
    "\n",
    "- Model and tokenizer loading\n",
    "- Proper handling of preference/comparison data\n",
    "- Inference behavior aligned with therapeutic and empathetic objectives\n",
    "- Environment and dependency correctness\n",
    "\n",
    "Although intentionally minimal, this notebook serves as a starting point for:\n",
    "\n",
    "- Preference-based fine-tuning approaches (e.g., DPO-style methods)\n",
    "- Behavioral alignment and safety evaluation\n",
    "- Prompt engineering in psychological and therapeutic contexts\n",
    "- Rapid experimentation prior to larger-scale alignment pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHait__RosVb"
   },
   "source": [
    "## 2Ô∏è‚É£ Environment & Dependencies\n",
    "\n",
    "This notebook assumes:\n",
    "- PyTorch with CUDA support\n",
    "- Hugging Face Transformers\n",
    "- bitsandbytes (for 4-bit quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers --quiet\n",
    "%pip install -U datasets --quiet\n",
    "%pip install -U accelerate --quiet\n",
    "%pip install -U peft --quiet\n",
    "%pip install -U trl --quiet\n",
    "%pip install -U bitsandbytes --quiet\n",
    "%pip install -U flash-attn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_token = userdata.get(\"HUGGINGFACE_TOKEN_GOOGLE_COLAB\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from typing import Tuple, Final, List, Set, Type, Dict, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"GPU CUDA not found\"\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME: Final[str] = \"google/gemma-2b-it\"\n",
    "DATASET_NAME: Final[str] = \"jkhedri/psychology-dataset\"\n",
    "DATA_SAMPLES: int | None = None\n",
    "NEW_MODEL: Final[str] = \"Gemma-2-it-Psych\"\n",
    "SYSTEM_PROMPT: Final[str] = \"You are a compassionate mental health assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmz4qFaSuxIq"
   },
   "source": [
    "## 3Ô∏è‚É£  GPU Optimization\n",
    "\n",
    "Checks CUDA compute capability to automatically select the most efficient data type (`bfloat16` vs `float16`) and attention mechanism (`Flash Attention 2` vs `Eager`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUDA] 7.5 ‚Üí torch.float16, eager\n"
     ]
    }
   ],
   "source": [
    "major, minor = torch.cuda.get_device_capability()\n",
    "\n",
    "torch_dtype, attn_implementation = (\n",
    "    (torch.bfloat16, \"flash_attention_2\")\n",
    "    if major >= 8\n",
    "    else (torch.float16, \"eager\")\n",
    ")\n",
    "\n",
    "print(f\"[CUDA] {major}.{minor} ‚Üí {torch_dtype}, {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLRFbfi1vDDa"
   },
   "source": [
    "## 4Ô∏è‚É£ Quantization Configuration\n",
    "\n",
    "Sets up 4-bit quantization using bitsandbytes (QLoRA). It uses **NF4** (NormalFloat4) and **Double Quantization** to maximize VRAM efficiency while maintaining model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTJTc7w0vOts"
   },
   "source": [
    "## 5Ô∏è‚É£ Load Model and Tokenizer\n",
    "\n",
    "Loads the pre-trained model with quantization (4-bit/8-bit) and configures the tokenizer for SFT (Supervised Fine-Tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e2254a06364e56beb7ed6c3a93e904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8d0de51f3b4cf399c59eb303ede4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1147eeefaed449eb6e92db1751a645e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8777143c0085451a9abc36f475ec0de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c2faa227ab46e293775dc1f1fe446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405591600ba84e4d8de4b4e37fcec14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5a6b8ef0c744329f4dd383f1aa9ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3daeef23de54d70a0b7fd8c5ca2b66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6555f1f5ff344a618f6d579f61f625da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e461670404d729445e628ccad5529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b6e9b959342ffbbcbd8655115f573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch_dtype,\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E56yYs4svYnB"
   },
   "source": [
    "## 6Ô∏è‚É£ Target Module Discovery\n",
    "\n",
    "Automatically identifies all 4-bit linear layers within the model architecture to apply LoRA adapters, excluding the output layer (`lm_head`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(\n",
    "    model: nn.Module,\n",
    "    linear_cls: Type[nn.Module] = bnb.nn.Linear4bit,\n",
    "    exclude: Set[str] | None = {\"lm_head\"},\n",
    ") -> List[str]:\n",
    "    names = {\n",
    "        name.split(\".\")[-1]\n",
    "        for name, module in model.named_modules()\n",
    "        if isinstance(module, linear_cls)\n",
    "    }\n",
    "\n",
    "    return sorted(names - (exclude or set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(f\"LoRA modules: {modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1OEBdcousTj"
   },
   "source": [
    "## 7Ô∏è‚É£ LoRA Adapter Configuration\n",
    "\n",
    "Configures the LoRA parameters (Rank and Alpha) and attaches the adapters to the target modules. It also displays the number of trainable parameters, showing the efficiency of PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDUx-6YfwGLm"
   },
   "source": [
    "## 8Ô∏è‚É£ Dataset Loading\n",
    "\n",
    "Loads and shuffles the dataset with a toggle for **Test Mode**. If `DATA_SAMPLES` is set, it limits the data to a small subset for fast iteration before running the full fine-tuning.\n",
    "\n",
    "Applies a standardized chat template to the training and test sets. This process:\n",
    "\n",
    "1. **Wraps** user questions and system prompts into the model's specific conversation format.\n",
    "2. **Pairs** them with the psychological responses (`response_j`).\n",
    "3. **Cleans** the dataset by removing raw columns and keeping only the formatted `text` for training.\n",
    "\n",
    "**Note:** This is a preference/comparison dataset where `response_j` (empathetic/therapeutic) and `response_k` (judgmental/aggressive) represent opposite poles.\n",
    "\n",
    "We are specifically selecting **`response_j`** for training to ensure the model learns safe, professional, and supportive psychological guidance while explicitly avoiding the toxic patterns found in `response_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa7a762568d4663af8bdbac6b42743c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/part-00000-694db9fd-774c-4205-b938-(‚Ä¶):   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da495e56fc4ce3b47397911c74082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/part-00001-694db9fd-774c-4205-b938-(‚Ä¶):   0%|          | 0.00/96.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cfc82356e84f6385a0e0ab586ea3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65)\n",
    "\n",
    "if DATA_SAMPLES is not None:\n",
    "    dataset = dataset.select(range(DATA_SAMPLES))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(\n",
    "    row: Dict[str, Any],\n",
    "    *,\n",
    "    tokenizer,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    ") -> Dict[str, Any]:\n",
    "    user_content = f\"{system_prompt}\\n\\n{row['question']}\"\n",
    "\n",
    "    messages = (\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response_j\"]},\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **row,\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9841dcc4f04f46bebea63f9db32be300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71902b7505564343b07507828a11c23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_split(split: Dataset) -> Dataset:\n",
    "    return split.map(\n",
    "        lambda row: format_chat_template(row, tokenizer=tokenizer),\n",
    "        remove_columns=split.column_names,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "dataset[\"train\"] = map_split(dataset[\"train\"])\n",
    "dataset[\"test\"] = map_split(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'response_j', 'response_k', 'text'],\n",
       "        num_rows: 8861\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'response_j', 'response_k', 'text'],\n",
       "        num_rows: 985\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmYll9nf-ahe"
   },
   "source": [
    "## 9Ô∏è‚É£ Training\n",
    "\n",
    "Defines the execution strategy, including **Gradient Accumulation** to simulate larger batches on limited VRAM, **Paged AdamW** optimizer for memory stability, and logging/evaluation intervals to monitor the model's psychological alignment.\n",
    "\n",
    "After initializes the **SFTTrainer** by combining the model, processed dataset, LoRA configuration, and training arguments. This step starts the supervised fine-tuning process, optimizing the model to generate empathetic psychological responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=NEW_MODEL,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79aaf7a8660a4067b9625f81b33abe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b6923924f946db9d89deda87a124f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceef6c083534fd4ac084ce127d834bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/8861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6045d8f663bf4bf7b99515108b27687b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a62d6e7fb59462f9ff402dc5963865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698e6dd02fb04516955d810ac9e7a4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='942' max='4431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 942/4431 49:18 < 3:03:01, 0.32 it/s, Epoch 0.21/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.826200</td>\n",
       "      <td>0.820603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.765500</td>\n",
       "      <td>0.898811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>0.776212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.786854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkX5HqH0-v9-"
   },
   "source": [
    "## 1Ô∏è‚É£0Ô∏è‚É£ Training Visualization\n",
    "\n",
    "Initializes **TensorBoard** to monitor training metrics in real-time. This allows for tracking the loss curve and ensuring the model is converging correctly during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./Gemma-2-it-Psych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eULmfvrv_4oV"
   },
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Saving and Exporting the Model\n",
    "\n",
    "Persists the fine-tuned LoRA adapters to local storage and uploads the final weights to the **Hugging Face Hub**. This ensures the model is versioned and ready for deployment or future inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(NEW_MODEL)\n",
    "trainer.model.push_to_hub(NEW_MODEL, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
