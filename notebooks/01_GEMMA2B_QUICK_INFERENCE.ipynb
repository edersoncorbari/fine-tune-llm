{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFqkDeQ6nOC6"
   },
   "source": [
    "# üìò Gemma 2B ‚Äì Quick Inference\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** January 24, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **quick and lightweight smoke test** for loading and running inference with the **Gemma 2B Large Language Model (LLM)**.\n",
    "\n",
    "The primary goal is to validate:\n",
    "- Model loading and tokenizer setup\n",
    "- Basic text generation (inference)\n",
    "- Environment and dependency correctness\n",
    "\n",
    "This notebook is intentionally minimal and designed for **rapid validation**, serving as a starting point for:\n",
    "- Fine-tuning experiments\n",
    "- Prompt engineering\n",
    "- Performance and behavior testing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cLDxnooXEC"
   },
   "source": [
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "This notebook validates that a Large Language Model (LLM) can be:\n",
    "- Loaded correctly (with optional 4-bit quantization)\n",
    "- Placed on the appropriate device (GPU)\n",
    "- Used to perform a simple inference task\n",
    "\n",
    "The goal is not benchmarking, but ensuring the runtime, model, and tokenizer\n",
    "are correctly configured and operational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHait__RosVb"
   },
   "source": [
    "## 2Ô∏è‚É£ Environment & Dependencies\n",
    "\n",
    "This notebook assumes:\n",
    "- PyTorch with CUDA support\n",
    "- Hugging Face Transformers\n",
    "- bitsandbytes (for 4-bit quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes --quiet\n",
    "%pip install -U transformers --quiet\n",
    "%pip install -U accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_token = userdata.get(\"HUGGINGFACE_TOKEN_GOOGLE_COLAB\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"GPU CUDA not found\"\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gksqzwUpwis"
   },
   "source": [
    "## 3Ô∏è‚É£ Utility Functions (Model Loading)\n",
    "\n",
    "The following helpers automatically:\n",
    "- Select the best compute dtype\n",
    "- Enable 4-bit quantization when requested\n",
    "- Load the tokenizer and model safely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Final\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_compute_dtype() -> torch.dtype:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    return torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(\n",
    "    model_name: str,\n",
    "    quantized: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
    "    quant_config: Optional[BitsAndBytesConfig] = None\n",
    "\n",
    "    if quantized:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=best_compute_dtype(),\n",
    "        )\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCD0vxPCqZ3f"
   },
   "source": [
    "## 4Ô∏è‚É£ Load Model\n",
    "\n",
    "This section performs the actual model and tokenizer loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c879f5908146a39f4c07ab737f2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME: Final[str] = \"google/gemma-2b-it\"\n",
    "\n",
    "model, tokenizer = load_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    quantized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PeTM2hYrF6n"
   },
   "source": [
    "## 5Ô∏è‚É£ Inference Utilities\n",
    "\n",
    "This function builds a simple prompt, performs text generation,\n",
    "and renders the output as Markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT: Final[str] = (\n",
    "    \"Design a data architecture for a customer support platform with the goal \"\n",
    "    \"of generating actionable business insights. Include an ASCII diagram and \"\n",
    "    \"explain which metrics, datasets, and analyses would most influence product \"\n",
    "    \"and operational decisions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"System: {system}\n",
    "User: {user}\n",
    "AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    max_new_tokens: int = 1000,\n",
    "    temperature: float = 0.7,\n",
    ") -> None:\n",
    "    device = model.device\n",
    "    prompt = build_prompt(system_prompt, user_prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = decoded[len(prompt):].strip()\n",
    "\n",
    "    display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7nlDSiirnXs"
   },
   "source": [
    "## 6Ô∏è‚É£ Run Inference\n",
    "\n",
    "Execute a simple inference to validate the full pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a proposed data architecture for a customer support platform:\n",
       "\n",
       "**Data Warehouse**\n",
       "\n",
       "* **Source tables:**\n",
       "    * Customer Support Ticket (text, keywords, timestamps)\n",
       "    * Support Ticket Resolutions (text, keywords, timestamps)\n",
       "    * Customer Demographics (demographic information, purchase history)\n",
       "* **Transformations:**\n",
       "    * Create a fact table for each source table\n",
       "    * Join relevant tables to enrich data\n",
       "\n",
       "**Operational Data Store**\n",
       "\n",
       "* **Source tables:**\n",
       "    * Support Ticket (text, keywords, timestamps)\n",
       "    * Support Ticket Resolutions (text, keywords, timestamps)\n",
       "    * Customer Demographics (demographic information, purchase history)\n",
       "    * Customer Support KPIs (resolution rate, average resolution time)\n",
       "* **Transformations:**\n",
       "    * Extract key metrics and generate reports\n",
       "\n",
       "**Data Analytics Platform**\n",
       "\n",
       "* **Data Lake:**\n",
       "    * Raw data from various sources\n",
       "    * Historical data for analysis\n",
       "* **Data Discovery Tools**\n",
       "    * Explore, analyze, and discover insights\n",
       "* **Business Intelligence Tools**\n",
       "    * Dashboards, reports, and charts for insights\n",
       "* **Performance Monitoring Tools**\n",
       "    * Track data quality, performance metrics, and alerts\n",
       "\n",
       "**Data Flow**\n",
       "\n",
       "1. Raw data is collected from various sources and loaded into the data warehouse.\n",
       "2. Data transformations clean and prepare data for analysis.\n",
       "3. Data is loaded into the operational data store.\n",
       "4. Business intelligence tools analyze data for insights and generate reports.\n",
       "5. Performance monitoring tools track data quality and performance.\n",
       "\n",
       "**ASCII Diagram**\n",
       "\n",
       "```\n",
       "Data Warehouse\n",
       "|-----> Fact Table (Customer Support Ticket)\n",
       "|-----> Source Table (Customer Support Ticket)\n",
       "|-----> Source Table (Support Ticket Resolutions)\n",
       "|-----> Source Table (Customer Demographics)\n",
       "|-----> Data Lake\n",
       "|-----> Data Discovery Tools\n",
       "|-----> Data Analytics Platform\n",
       "|-----> Data Visualization Tools\n",
       "```\n",
       "\n",
       "**Metrics and Data Sets that Most Influence Product and Operational Decisions**\n",
       "\n",
       "* **Customer satisfaction**: Customer satisfaction surveys, feedback analysis, and support ticket resolution data.\n",
       "* **Support ticket resolution time**: Average time taken to resolve tickets, identify bottlenecks, and optimize support processes.\n",
       "* **Customer churn rate**: Number of customers who stop using the service.\n",
       "* **Net Promoter Score (NPS)**: A measure of customer loyalty and willingness to recommend the service.\n",
       "* **Customer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_prompt=USER_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21GZQaRUr6Yf"
   },
   "source": [
    "## 7Ô∏è‚É£ Validation Checklist\n",
    "\n",
    "- [x] Model loads without errors\n",
    "- [x] Tokenizer is correctly configured\n",
    "- [x] Device placement is correct\n",
    "- [x] Inference produces coherent output\n",
    "- [x] Markdown rendering works as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIvXJhcxsOD2"
   },
   "source": [
    "## ‚úÖ Final Notes\n",
    "\n",
    "This notebook serves as a reusable smoke test for:\n",
    "- New models\n",
    "- New environments\n",
    "- Quantization configurations\n",
    "- Runtime changes\n",
    "\n",
    "It can be extended with benchmarking, streaming, or structured outputs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
