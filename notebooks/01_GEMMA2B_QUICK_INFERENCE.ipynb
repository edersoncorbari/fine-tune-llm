{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFqkDeQ6nOC6"
   },
   "source": [
    "# üìò Gemma 2B ‚Äì Quick Inference\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** January 24, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **quick and lightweight smoke test** for loading and running inference with the **Gemma 2B Large Language Model (LLM)**.\n",
    "\n",
    "The primary goal is to validate:\n",
    "- Model loading and tokenizer setup\n",
    "- Basic text generation (inference)\n",
    "- Environment and dependency correctness\n",
    "\n",
    "This notebook is intentionally minimal and designed for **rapid validation**, serving as a starting point for:\n",
    "- Fine-tuning experiments\n",
    "- Prompt engineering\n",
    "- Performance and behavior testing\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cLDxnooXEC"
   },
   "source": [
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "This notebook validates that a Large Language Model (LLM) can be:\n",
    "- Loaded correctly (with optional 4-bit quantization)\n",
    "- Placed on the appropriate device (GPU)\n",
    "- Used to perform a simple inference task\n",
    "\n",
    "The goal is not benchmarking, but ensuring the runtime, model, and tokenizer\n",
    "are correctly configured and operational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHait__RosVb"
   },
   "source": [
    "## 2Ô∏è‚É£ Environment & Dependencies\n",
    "\n",
    "This notebook assumes:\n",
    "- PyTorch with CUDA support\n",
    "- Hugging Face Transformers\n",
    "- bitsandbytes (for 4-bit quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes --quiet\n",
    "%pip install -U transformers --quiet\n",
    "%pip install -U accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA T1000 8GB\n",
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"GPU CUDA not found\"\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gksqzwUpwis"
   },
   "source": [
    "## 3Ô∏è‚É£ Utility Functions (Model Loading)\n",
    "\n",
    "The following helpers automatically:\n",
    "- Select the best compute dtype\n",
    "- Enable 4-bit quantization when requested\n",
    "- Load the tokenizer and model safely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Final\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_compute_dtype() -> torch.dtype:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    return torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(\n",
    "    model_name: str,\n",
    "    quantized: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
    "    quant_config: Optional[BitsAndBytesConfig] = None\n",
    "\n",
    "    if quantized:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=best_compute_dtype(),\n",
    "        )\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device_map,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCD0vxPCqZ3f"
   },
   "source": [
    "## 4Ô∏è‚É£ Load Model\n",
    "\n",
    "This section performs the actual model and tokenizer loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5b74bd56e9403c956d5cce6d6a004f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME: Final[str] = \"google/gemma-2b-it\"\n",
    "\n",
    "model, tokenizer = load_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    quantized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PeTM2hYrF6n"
   },
   "source": [
    "## 5Ô∏è‚É£ Inference Utilities\n",
    "\n",
    "This function builds a simple prompt, performs text generation,\n",
    "and renders the output as Markdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT: Final[str] = (\n",
    "    \"You are a senior data architect focused on practical, high-impact analytics \"\n",
    "    \"for customer support platforms.\\n\\n\"\n",
    "    \"Your task is to design a clear and simple data architecture that enables \"\n",
    "    \"actionable business insights.\\n\\n\"\n",
    "    \"Follow these rules strictly:\\n\"\n",
    "    \"- Keep explanations concise and structured.\\n\"\n",
    "    \"- Cover the full data flow: sources ‚Üí ingestion ‚Üí storage ‚Üí analytics ‚Üí consumption.\\n\"\n",
    "    \"- Always link architecture choices to business decisions.\\n\"\n",
    "    \"- Use an ASCII diagram to show the architecture.\\n\"\n",
    "    \"- Explicitly list:\\n\"\n",
    "    \"  1. Key datasets\\n\"\n",
    "    \"  2. Key metrics\\n\"\n",
    "    \"  3. Key analyses and the decisions they enable\\n\"\n",
    "    \"- Prefer clarity over completeness; avoid deep implementation details.\\n\\n\"\n",
    "    \"Use plain, unambiguous language.\\n\"\n",
    "    \"Do not include unnecessary background or theory.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT: Final[str] = (\n",
    "    \"Design a data architecture for a customer support platform with the goal \"\n",
    "    \"of generating actionable business insights. Include an ASCII diagram and \"\n",
    "    \"explain which metrics, datasets, and analyses would most influence product \"\n",
    "    \"and operational decisions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(system: str, user: str) -> str:\n",
    "    return f\"\"\"System: {system}\n",
    "User: {user}\n",
    "AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inference(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    max_new_tokens: int = 1000,\n",
    "    temperature: float = 0.7,\n",
    ") -> None:\n",
    "    device = model.device\n",
    "    prompt = build_prompt(system_prompt, user_prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = decoded[len(prompt):].strip()\n",
    "\n",
    "    display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7nlDSiirnXs"
   },
   "source": [
    "## 6Ô∏è‚É£ Run Inference\n",
    "\n",
    "Execute a simple inference to validate the full pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The data architecture will be designed to provide actionable insights for customer support platforms. It will consist of a central data warehouse that ingests data from various sources, including customer support logs, social media, and CRM systems. The data will be stored in a structured format and analyzed using advanced analytics tools to identify patterns, trends, and correlations. These insights will be used to improve customer support processes, identify areas for product improvement, and make data-driven decisions.\n",
       "\n",
       "**Key Datasets**\n",
       "- Customer Support Logs\n",
       "- Social Media Data\n",
       "- CRM Data\n",
       "\n",
       "**Key Metrics**\n",
       "- First Resolution Rate\n",
       "- Average Resolution Time\n",
       "- Support Tickets Opened by Channel\n",
       "- Customer Satisfaction Rating\n",
       "- Number of Support Tickets Resolved\n",
       "\n",
       "**Key Analyses and Decisions**\n",
       "- Analyze trends in customer support data to identify areas for improvement.\n",
       "- Identify the most common issues that customers encounter.\n",
       "- Analyze sentiment of customer feedback to understand their level of satisfaction and identify areas for improvement.\n",
       "- Identify opportunities to reduce resolution time and improve customer satisfaction.\n",
       "- Identify areas for product improvement to enhance the quality of customer support.\n",
       "\n",
       "**Architecture Diagram**\n",
       "\n",
       "```\n",
       "[Data Warehouse]\n",
       "|\n",
       "[Ingestion]\n",
       " |\n",
       "[Customer Support Logs]\n",
       " |\n",
       "[Social Media Data]\n",
       " |\n",
       "[CRM Data]\n",
       "|\n",
       "[Storage]\n",
       " |\n",
       "[Analytics]\n",
       " |\n",
       "[Consumption]\n",
       "```\n",
       "\n",
       "**Key Datasets**\n",
       "\n",
       "* **Customer Support Logs:** Contains records of customer interactions with the support team, including the time of the call, the issue reported, the resolution provided, and the outcome.\n",
       "* **Social Media Data:** Comprises posts and comments from customers on various social media platforms, including Facebook, Twitter, and LinkedIn.\n",
       "* **CRM Data:** Includes information about customers, such as their name, contact details, purchase history, and support tickets they have opened.\n",
       "\n",
       "**Key Metrics**\n",
       "\n",
       "* **First Resolution Rate:** The percentage of customer support tickets that are resolved on the first contact.\n",
       "* **Average Resolution Time:** The average amount of time taken to resolve a support ticket.\n",
       "* **Support Tickets Opened by Channel:** The number of customer support tickets opened through different channels, such as email, phone, chat, or social media.\n",
       "* **Customer Satisfaction Rating:** A survey asking customers how satisfied they are with the support they receive.\n",
       "* **Number of Support Tickets Resolved:** The total number of support tickets that have been resolved.\n",
       "\n",
       "**Key Analyses and Decisions**\n",
       "\n",
       "* Analyze trends in customer support data to identify areas for improvement.\n",
       "* Identify the most common issues that customers encounter.\n",
       "* Analyze sentiment of customer feedback to understand their level of satisfaction and identify areas for improvement.\n",
       "* Identify opportunities to reduce resolution time and improve customer satisfaction.\n",
       "* Identify areas for product improvement to enhance the quality of customer support."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_prompt=USER_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21GZQaRUr6Yf"
   },
   "source": [
    "## 7Ô∏è‚É£ Validation Checklist\n",
    "\n",
    "- [x] Model loads without errors\n",
    "- [x] Tokenizer is correctly configured\n",
    "- [x] Device placement is correct\n",
    "- [x] Inference produces coherent output\n",
    "- [x] Markdown rendering works as expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIvXJhcxsOD2"
   },
   "source": [
    "## ‚úÖ Final Notes\n",
    "\n",
    "This notebook serves as a reusable smoke test for:\n",
    "- New models\n",
    "- New environments\n",
    "- Quantization configurations\n",
    "- Runtime changes\n",
    "\n",
    "It can be extended with benchmarking, streaming, or structured outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
