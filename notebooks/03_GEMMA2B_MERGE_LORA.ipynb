{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFqkDeQ6nOC6"
   },
   "source": [
    "# üìò Gemma 2B ‚Äì Merge LoRA\n",
    "\n",
    "- **Author:** Ederson Corbari <e@NeuroQuest.ai>\n",
    "- **Date:** February 01, 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for loading, merging, and running inference with the **Gemma 2B Instruction-Tuned Large Language Model (LLM)** augmented with a **LoRA adapter trained on a psychological preference dataset**.\n",
    "\n",
    "The notebook focuses on:\n",
    "- Loading the base Gemma 2B instruction model\n",
    "- Attaching and merging a LoRA adapter for behavioral alignment\n",
    "- Exporting and publishing the merged model\n",
    "- Running a lightweight inference sanity check\n",
    "\n",
    "Rather than performing training inside the notebook, this workflow validates the **model integration, merging process, and inference behavior**, ensuring the resulting model is ready for downstream use in psychologically safe and empathetic conversational settings.\n",
    "\n",
    "The final merged model is publicly available on the Hugging Face Hub at:\n",
    "\n",
    "- **https://huggingface.co/ecorbari/Gemma-2b-it-Psych-Merged**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cLDxnooXEC"
   },
   "source": [
    "## 1Ô∏è‚É£ Introduction\n",
    "\n",
    "This notebook provides a minimal and practical pipeline for **merging a LoRA-adapted Gemma 2B model and validating it through inference**.\n",
    "\n",
    "The LoRA adapter used in this workflow was trained separately using preference-based data designed to encourage psychologically safe, empathetic, and therapeutically aligned responses. Here, the focus is on correctly loading the base model, applying the adapter, merging the weights, and verifying that the final model behaves as expected during inference.\n",
    "\n",
    "The primary objectives of this notebook are to:\n",
    "\n",
    "- Load the Gemma 2B instruction-tuned base model efficiently\n",
    "- Attach and merge a LoRA adapter into the base model\n",
    "- Export and publish the merged model for reuse\n",
    "- Perform a lightweight inference test to validate behavior and environment setup\n",
    "\n",
    "This notebook is intentionally lightweight and serves as a **validation and deployment step**, rather than a full training pipeline. It is well-suited for rapid iteration, sanity checks, and preparation of aligned models for downstream applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHait__RosVb"
   },
   "source": [
    "## 2Ô∏è‚É£ Environment & Dependencies\n",
    "\n",
    "This notebook assumes:\n",
    "- PyTorch with CUDA support\n",
    "- Hugging Face Transformers\n",
    "- bitsandbytes (for 4-bit quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(\"../.env\")\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Final\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA T1000 8GB\n",
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"GPU CUDA not found\"\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL: Final[str] = \"google/gemma-2b-it\"\n",
    "LORA_MODEL: Final[str] = \"ecorbari/Gemma-2b-it-Psych\"  \n",
    "MERGED_MODEL: Final[str] = \"Gemma-2b-it-Psych-Merged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Base Model\n",
    "\n",
    "Loads a causal LLM with automatic device placement and reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614d17cbf0f14807b22f4233aacc460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load Tokenizer\n",
    "\n",
    "Loads the tokenizer and uses the EOS token as the padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load and Merge LoRA Adapter\n",
    "\n",
    "Loads the LoRA fine-tuned adapters on top of the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LORA_MODEL,\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Save Merged Model\n",
    "\n",
    "Saves the merged model weights and tokenizer to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1617f329fd462fb031f08a32ccb496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Gemma-2b-it-Psych-Merged/tokenizer_config.json',\n",
       " 'Gemma-2b-it-Psych-Merged/chat_template.jinja',\n",
       " 'Gemma-2b-it-Psych-Merged/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(MERGED_MODEL)\n",
    "tokenizer.save_pretrained(MERGED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Saving and Exporting the Model\n",
    "\n",
    "Uploads the merged model and tokenizer to the [Hugging Face Hub](https://huggingface.co/ecorbari/Gemma-2b-it-Psych-Merged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69658061e7741568b3067c855b8a0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a99db1caf544a87a873996d834b5fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04de9388b09a4b2dbfde7a6ab5920372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279c8d554b5d46ee90517c739ab44b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c069da526b4024bbd6fd3d77c23ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ecorbari/Gemma-2b-it-Psych-Merged/commit/5e3daaf83c75a5234d00e59f7c310007711883b5', commit_message='Upload tokenizer', commit_description='', oid='5e3daaf83c75a5234d00e59f7c310007711883b5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ecorbari/Gemma-2b-it-Psych-Merged', endpoint='https://huggingface.co', repo_type='model', repo_id='ecorbari/Gemma-2b-it-Psych-Merged'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(MERGED_MODEL)\n",
    "tokenizer.push_to_hub(MERGED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Inference Test\n",
    "\n",
    "Sets up a text-generation inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA T1000 8GB\n",
      "0.0 MB\n",
      "0.0 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(torch.cuda.memory_reserved() / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e8282052954c99bd742f4460d93a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MERGED_MODEL,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### ü§ñ Model Response\n",
       "\n",
       "model\n",
       "It's understandable to feel anxious and overwhelmed, especially in uncertain times. One strategy that could help is deep breathing exercises. Take slow, deep breaths and focus on your breath. You can also try progressive muscle relaxation, which involves gradually tensing and relaxing different muscle groups in your body. Additionally, it may be helpful to identify the triggers that cause your anxiety and work on developing coping strategies to manage them. Remember to be kind and compassionate with yourself, and seek support from loved ones or a therapist if necessary.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "prompt = \"I feel anxious and overwhelmed lately. What should I do?\"\n",
    "result = pipe(prompt)\n",
    "\n",
    "response = result[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "display(\n",
    "    Markdown(f\"\"\"\n",
    "### ü§ñ Model Response\n",
    "\n",
    "{response}\n",
    "\"\"\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
